{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl_links():\n",
    "    dl_list =[]\n",
    "    files = os.listdir(\"combined\")\n",
    "    for i in range(len(files)):\n",
    "        with open(\"combined/\" + files[i], \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            for j in range(len(data)):\n",
    "                date = data[j][\"date\"].split(\" \")\n",
    "                date.remove(date[-1])\n",
    "                date = date[0].replace(\"-\", \"/\")\n",
    "                img = data[j][\"image\"] + \".png\"\n",
    "                link = f\"https://epic.gsfc.nasa.gov/archive/enhanced/{date}/png/{img}\"\n",
    "                dl_list.append(link)\n",
    "    return dl_list\n",
    "list = create_dl_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SummaryWriter instance\n",
    "writer = SummaryWriter(f\"runs/autoencoder/{datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            ## nn.Identity(), # Does nothing\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=8,  stride=2, padding=3), # Could be: Padding = (kernel_size - 1) / 2 to retain input size\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=8, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=8, stride=2, padding=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            ## nn.Identity(), # Does nothing\n",
    "            # now trying to reverse the encoder\n",
    "            nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=8, stride=2, padding=3),\n",
    "            nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=8, stride=2, padding=3),\n",
    "            nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=8, stride=2, padding=3),          \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with metrics and hyperparameters here:\n",
    "hparam_dict = {\n",
    "    \"device\": torch.device(\"mps\" if torch.backends.mps.is_available() else \"mps\"), # Change to \"cuda\" if you want to use use Nvidia GPU\n",
    "    \"batch_size\": 1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 5,\n",
    "    \"use_batch_download\": False, # DANGER! If set to True, will delete entire dataset folder at end of training!\n",
    "    \"loader_workers\": 1,\n",
    "    \"transform\": transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "metric_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "autoencoder = Autoencoder().to(hparam_dict[\"device\"])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=hparam_dict[\"learning_rate\"])\n",
    "\n",
    "# Add criterion and optimizer to hparam_dict\n",
    "# Use string and split by \".\" to get class name\n",
    "\n",
    "hparam_dict[\"criterion\"] = str(criterion.__class__).split(\".\")[-1][:-2]\n",
    "hparam_dict[\"optimizer\"] = str(optimizer.__class__).split(\".\")[-1][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Autoencoder                              [1, 3, 512, 512]          --\n",
      "├─Sequential: 1-1                        [1, 3, 64, 64]            --\n",
      "│    └─Conv2d: 2-1                       [1, 3, 256, 256]          579\n",
      "│    └─ReLU: 2-2                         [1, 3, 256, 256]          --\n",
      "│    └─Conv2d: 2-3                       [1, 3, 128, 128]          579\n",
      "│    └─ReLU: 2-4                         [1, 3, 128, 128]          --\n",
      "│    └─Conv2d: 2-5                       [1, 3, 64, 64]            579\n",
      "│    └─ReLU: 2-6                         [1, 3, 64, 64]            --\n",
      "├─Sequential: 1-2                        [1, 3, 512, 512]          --\n",
      "│    └─ConvTranspose2d: 2-7              [1, 3, 128, 128]          579\n",
      "│    └─ConvTranspose2d: 2-8              [1, 3, 256, 256]          579\n",
      "│    └─ConvTranspose2d: 2-9              [1, 3, 512, 512]          579\n",
      "==========================================================================================\n",
      "Total params: 3,474\n",
      "Trainable params: 3,474\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 249.02\n",
      "==========================================================================================\n",
      "Input size (MB): 3.15\n",
      "Forward/backward pass size (MB): 10.32\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 13.48\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(autoencoder, input_size=(1, 3, 512, 512))) # Static\n",
    "writer.add_graph(autoencoder, torch.rand(1, 3, 512, 512)) # Static\n",
    "\n",
    "# Start tensorboard\n",
    "tensorboard_process = subprocess.Popen([\"tensorboard\", \"--logdir=runs\", \"--port=6006\",], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch browser on tensorboard port\n",
    "webbrowser.open(\"http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No truncated images found.\n"
     ]
    }
   ],
   "source": [
    "# Checking for truncated images if use_batch_download is False\n",
    "if not hparam_dict[\"use_batch_download\"]:\n",
    "    def is_image_truncated(file_path):\n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(file_path)\n",
    "            img.verify()  # Verify the integrity of the image file\n",
    "            return False    # Image is not truncated\n",
    "        except (IOError, SyntaxError):\n",
    "            return True     # Image is truncated\n",
    "\n",
    "    def find_truncated_images(folder_path):\n",
    "        truncated_images = []\n",
    "\n",
    "        # Iterate over files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Check if the file is a regular file and not a directory\n",
    "            if os.path.isfile(file_path):\n",
    "                if is_image_truncated(file_path):\n",
    "                    truncated_images.append(filename)\n",
    "\n",
    "        return truncated_images\n",
    "\n",
    "    # Example: Replace 'your_folder_path' with the actual path to your folder\n",
    "    folder_path = 'download/earth/'\n",
    "    truncated_images = find_truncated_images(folder_path)\n",
    "\n",
    "    if truncated_images:\n",
    "        print(\"Truncated Images:\")\n",
    "        for image in truncated_images:\n",
    "            print(image)\n",
    "    else:\n",
    "        print(\"No truncated images found.\")\n",
    "\n",
    "    # Remove truncated images\n",
    "    for image in truncated_images:\n",
    "        os.remove(os.path.join(folder_path, image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hparam_dict, metric_dict):\n",
    "    # Make sure the model is on the right device\n",
    "    autoencoder.to(hparam_dict[\"device\"]) # Else this happens: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same (RuntimeError)\n",
    "    epoch_time_list = []\n",
    "\n",
    "    for epoch in track(range(hparam_dict[\"num_epochs\"]), description=f'Starting training loop... '):\n",
    "        if hparam_dict[\"use_batch_download\"]:\n",
    "            # Download data and preprocess\n",
    "            for i in range(hparam_dict[\"batch_size\"]):\n",
    "                # Download data\n",
    "                # Pick random image from list\n",
    "                rand = random.randint(0, len(list))\n",
    "                os.system(f\"wget -P download/earth {list[rand]} --quiet\")\n",
    "        epoch_start = time.time()\n",
    "        # Load data\n",
    "        dataset = ImageFolder(root='download', transform=hparam_dict[\"transform\"])\n",
    "        dataloader = DataLoader(dataset, batch_size=hparam_dict[\"batch_size\"], shuffle=True, num_workers=hparam_dict[\"loader_workers\"])\n",
    "\n",
    "        for data in dataloader:\n",
    "            input, _ = data\n",
    "            input = input.to(hparam_dict[\"device\"])\n",
    "\n",
    "            # Forward pass\n",
    "            output = autoencoder(input)\n",
    "            loss = criterion(output, input)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Log loss to tensorboard\n",
    "        writer.add_scalar(\"Reconstruction loss\", loss.item(), epoch, time.time())\n",
    "\n",
    "        # Prepare the last reconstructed image \n",
    "        detached_tensor = output.detach().cpu()\n",
    "\n",
    "        # Save the input and output images\n",
    "        writer.add_image(\"Input\", make_grid(input), epoch)\n",
    "        writer.add_image(\"Output\", make_grid(detached_tensor), epoch)\n",
    "\n",
    "        # Clear download folder\n",
    "        if hparam_dict[\"use_batch_download\"]:\n",
    "            os.system(\"rm download/earth/*\")\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "\n",
    "        # Convert to minutes\n",
    "        epoch_time = epoch_time / 60\n",
    "\n",
    "        # Add epoch_time to epoch_time_list\n",
    "        epoch_time_list.append(epoch_time)\n",
    "\n",
    "        # Log epoch time to tensorboard\n",
    "        writer.add_scalar(\"Epoch time (minutes)\", epoch_time, epoch)\n",
    "        \n",
    "\n",
    "\n",
    "    # Calculate average epoch time\n",
    "    avg_epoch_time = sum(epoch_time_list) / len(epoch_time_list)\n",
    "\n",
    "    # Add avg_epoch_time to metric_dict\n",
    "    metric_dict[\"Average epoch time (minutes)\"] = avg_epoch_time\n",
    "\n",
    "    # Add total training time to metric_dict\n",
    "    metric_dict[\"Total training time (minutes)\"] = sum(epoch_time_list)\n",
    "\n",
    "    # Add loss to metric_dict\n",
    "    metric_dict[\"Last reconstruction loss\"] = loss.item()\n",
    "\n",
    "    # Configure hparam_dict and metric_dict for tensorboard\n",
    "    hparam_dict = {k: str(v) for k, v in hparam_dict.items()}\n",
    "    metric_dict = {k: float(v) for k, v in metric_dict.items()}\n",
    "\n",
    "    # Add hparam_dict and metric_dict to tensorboard\n",
    "    writer.add_hparams(hparam_dict, metric_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64856e9ccffd41709078b129cb5c4542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train with hparam_dict as is at top of file\n",
    "train(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "# Train with hparam_dict modified\n",
    "hparam_dict[\"batch_size\"] = 2\n",
    "hparam_dict[\"loader_workers\"] = 2\n",
    "\n",
    "train(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "# Train with hparam_dict modified\n",
    "hparam_dict[\"batch_size\"] = 4\n",
    "hparam_dict[\"loader_workers\"] = 4\n",
    "\n",
    "train(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "# Train with hparam_dict modified\n",
    "hparam_dict[\"loader_workers\"] = 6\n",
    "hparam_dict[\"batch_size\"] = 6\n",
    "\n",
    "train(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "# Train with hparam_dict modified\n",
    "hparam_dict[\"batch_size\"] = 8\n",
    "hparam_dict[\"loader_workers\"] = 8\n",
    "\n",
    "train(hparam_dict=hparam_dict, metric_dict=metric_dict)\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate tensorboard\n",
    "tensorboard_process.terminate()\n",
    "\n",
    "# Wait for termination\n",
    "tensorboard_process.wait()\n",
    "\n",
    "# Get the return code\n",
    "return_code = tensorboard_process.returncode\n",
    "print(f\"Return code of tensorboard subprocess: {return_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
