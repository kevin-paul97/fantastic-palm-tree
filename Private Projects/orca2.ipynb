{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80134d601cf9476280f17770265acfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0a053a5e0148f3b05f04a5e1e6b833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030ca47a995d4e1c8f00b10a7cc3bbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    torch.set_default_device(\"cpu\")\n",
    "    \n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-7b\", device_map='auto')\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/27132\n",
    "# please use the slow tokenizer since fast and slow tokenizer produces different tokens\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/Orca-2-7b\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n",
    "user_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n",
    "\n",
    "prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output_ids = model.generate(inputs[\"input_ids\"],)\n",
    "answer = tokenizer.batch_decode(output_ids)[0]\n",
    " \n",
    "print(answer)\n",
    "\n",
    "# This example continues showing how to add a second turn message by the user to the conversation\n",
    "second_turn_user_message = \"Give me a list of the key points of your first answer.\"\n",
    "\n",
    "# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\n",
    "second_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "second_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\n",
    "second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n",
    "\n",
    "output_ids_2 = model.generate(second_turn_input,)\n",
    "second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n",
    "\n",
    "print(second_turn_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
